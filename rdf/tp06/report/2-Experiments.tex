\section{Visualising data}
\paragraph{}
The data we are going to use is present in a file of type \lstinline{Rdata} and it consists of two parts: data for \emph{training} and data for \emph{testing}.
Each dataset has a number of $75$ instances (an instance being described by $2$ \emph{features}) which are divided (not equally) amongst $3$ \emph{classes}.

\paragraph{}
Having a \emph{true class} assigned to each instance, we can treat this problem as a \emph{supervised classification} problem.
Moreover, we also have information about true \emph{means} and \emph{standard deviations}, so we can use these to make comparisons between our algorithms and the truth.

\paragraph{}
Let's take a look at how the data looks:

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth/2 - 10pt]{train_data.png}
    \includegraphics[width=\textwidth/2 - 10pt]{test_data.png}
    \caption{Plotted data. Train on left, test on right}
    \label{data}
\end{figure}

\begin{lstlisting}[language=R, caption=Loading and visualising data]
data <- load(file='simul-2017.Rdata')
class_colors = c('red', 'green', 'blue')
couleur_app <- rep('red', n_app)
for (class_no in 1:3)
  couleur_app[classe_app == class_no] = class_colors[class_no]
plot (x_app, col=couleur_app, main='Training data', xlab='1st feature', ylab='2nd feature')
for (class_no in 1:3)
  couleur_app[classe_test == class_no] = class_colors[class_no]
plot (x_test, col=couleur_app, main='Test data', xlab='1st feature', ylab='2nd feature')
\end{lstlisting}

\clearpage

\paragraph{}
We can notice more blue dots, which would mean instances are not equally distributed amongst classes.
Calculating the class a priori probabilities, that is indeed true:
\begin{lstlisting}[language=R, caption=A priori class probabilities]
# P(w) = probability of an instance to be in the class "w"
class_probs_app <- 1:3
class_probs_test <- 1:3
for (class_no in 1:3){
    class_probs_app[class_no] = sum(class_no == classe_app) / length(classe_app)
    class_probs_test[class_no] = sum(class_no == classe_test) / length(classe_test)
}
class_probs_app
class_probs_test
\end{lstlisting}

\paragraph{}
The script above gives us the values $P(red) = 0.28$, $P(green) = 0.3466667$ and $P(blue) = 0.3733333$ for the training dataset and $P(red) = 0.2666667$, $P(green) = 0.2666667$ and $P(blue) = 0.4666667$, for the test dataset.
The values corresepond to red, green and blue classes, in this order.


\section{Estimating means and covariance}

\subsection{Calculating means}

\paragraph{}
A first small step to analysing the data would be to look at the ``centroids'' for each class, that is calculating the means for each class.

\begin{lstlisting}[language=R, caption=Calculating means]
# calculate means
means <- array(dim = c(3, 2))
for (class_no in 1:3){
    means[class_no, 1] <- mean (x_app[classe_app == class_no, 1])
    means[class_no, 2] <- mean (x_app[classe_app == class_no, 2])
}
# plot the means for train data
plot (x_app, col=couleur_app, main='Means for train data', xlab='First feature', ylab='Second feature')
points (means, col=class_colors, p=8, cex=4)
\end{lstlisting}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{means_train.png}
    \caption{Visualising means with instances}
\end{figure}

\clearpage

\begin{center}
    \begin{tabular}{ cc }   % top level tables, with 2 rows
        True results & My results \\  
        % bottom left of the top level table: table 1 
        \begin{tabular}{ |c|c|c|c| } 
        \hline
        & Red & Green & Blue\\
        X1 & 8 & 10 & 4 \\
        X2 & 1 & 6 & 4 \\
        \hline
        \end{tabular} &  % starting bottom right of the top level table
        % table 2
        \begin{tabular}{ |c|c|c|c| } 
        \hline
        & Red & Green & Blue\\
        X1 & 7.951986 & 9.389311 & 4.365176 \\
        X2 & 1.237545 & 6.218550 & 3.874318 \\
        \hline
        \end{tabular} \\
    \end{tabular}
\end{center}

\paragraph{}
The values\footnote{The means were calculating only on the training dataset, not on all of the instances} aren't so distant from the ground truth, so we can consider them insightful.

\subsection{Covariance matrix}

\paragraph{}
Up next, we will calculate the covariance matrix.
According to \cite{covariance}, covariance is a measure of how one feature changes in relation to another.
Depending on the algorithm we're using, if two features are \emph{strongly} correlated, it might be better to let one go, in order to reduce the \emph{dimensionality} of the problem and make algorithms faster.
But, for example, algorithms like Naive Bayes might actually benefit from correlated features.
In our case, we only have $2$ features and $75$ instances, so the dimensionality of the problem doesn't really pose a problem for us.

\begin{lstlisting}[language=R, caption=Calculating covariance]
sigma <- c()
for (class_no in 1:3){
    # number of instances = n_app = dim(x_app)[1]
    # number of features = dim(x_app)[2]
    sigma[[class_no]] <- matrix(1, dim(x_app)[2], dim(x_app)[2])
    for (i in 1:dim(x_app)[2])
    for (j in 1:dim(x_app)[2])
        sigma[[class_no]][i, j] <- cov(as.vector(x_app[classe_app == class_no, i]), as.vector (x_app[classe_app == class_no, j]))
}
\end{lstlisting}

\paragraph{}
Below, we can find the script's result. For each class, we have calculated the \emph{covariance matrix}.
On the diagonal of each ${\Sigma}_i$ matrix, we have the \emph{variance} of each feature, and any other element ${\Sigma}_(j, k), j \neq k$, represents the correlation between the features $j$, $k$.
Of course, these matrixes are symmetric.

\[ 
    {\Sigma}_1 = 
    \left(\begin{array}{cc}
    0.95337723 & 0.09544548 \\
    0.09544548 & 3.26152120 \\
    \end{array} \right)
    , 
    {\Sigma}_2 = 
    \left( \begin{array}{cc}
    5.8526867 & 0.6107413 \\
    0.6107413 & 1.0474417 \\
    \end{array} \right)
    ,
\]
\[
    {\Sigma}_3 = 
    (\begin{array}{cc}
    2.0251769 & 0.7214545 \\
    0.7214545 & 3.0809579 \\
    \end{array})
\]

\[ 
    {\sigma}_1 = 
    \left(\begin{array}{cc}
    1 & 2 \\
    \end{array} \right)
    , 
    {\sigma}_2 = 
    \left( \begin{array}{cc}
    3 & 1 \\
    \end{array} \right)
    {\sigma}_3 = 
    \left( \begin{array}{cc}
    1.5 & 2 \\
    \end{array} \right)
\]

\paragraph{}
By ${\Sigma}_i$ we denoted the covariance matrix, and by ${\sigma}_i$ the ground truth regarding standard deviation.

\subsection{Analysing diagonal values}
\paragraph{}
Looking at the values on the diagonal and comparing them (! by not forgetting to raise the standard deviation, that is values from ${\sigma}_i$, to the power of 2, so we get variance), to the true values, we can see that overall they are pretty close.
The only exception is the variance for the first feature, for the second class; those aren't very similar ($\approx 5.8$ vs. $3^2$).
To cite \cite{variance}, variance ``measures how far a set of (random) numbers are spread out from their average value''.
That being said, it gives us a sense of dispersity in our data.

\paragraph{}
Now let's apply the above observations to our data\footnote{The covariance matrixes were calculated on the training data, so the observations we made are for that dataset}.
Before this, please note that in our plots above \ref{data}, the first feature $X_1$ was projected on the abscissa and the second feature $X_2$ on the ordinate.
Looking at the red class and variance for the first atribute, we could that, for the first feature, the variance is not that high.
That would mean that the red-class instances, projected on the x-axis, would be "gathered" around.
If we look at the image, that is, indeed, true! 

\paragraph{}
Thinking the other way around, if we look at the figure\ref{data} at $X_1$ for the green class, we can notice that the projections on the x-axis for the instances are quite dispersed.
That would mean that the variance for that case is higher.
And that is, once again, true (variance of $\approx 5.8$ in our results), proof that our algorithm makes the cut.

\subsection{Analysing correlation between features}
\paragraph{}
Let us now note ${\Sigma}_i(j, k) = x$, a correlation between two features.
Before analysing the values for $x$, we must take into account the following observations: if $x$ is positive, then if the feature $j$ increases, then so does the value for $k$.
If $x$ is negative, then if feature $j$ increases, then the value for $k$ decreases.
If $x$ is 0, then the features are not correlated.
We say that two features $j$ and $k$ are strongly correlated when the $x$ is large.

\paragraph{}
The covariance matrix tells us that for the train instances in the red class, there's almost no correlation, since the value is fairly low, $\approx 0.1$.
Even for the other classes, the correlation is still $<1$, so we can conclude that the features are not highly correlated.

\section{Linear discriminant analysis}

\subsection{Constructing the LDA model}
\paragraph{}
We will now try to use \emph{LDA} to find a linear combination between $X_1$ and $X_2$.
This algorithm will also allow us to construct \emph{boundaries} between classes.
We'll illustrate this as a Voronoi diagram.

\begin{lstlisting}[language=R, caption=Running LDA on train data]
# construct the grid
xp1 <- seq(min(x_app[,1]), max(x_app[,1]), length=50)
xp2 <- seq(min(x_app[,2]), max(x_app[,2]), length=50)
grille <- expand.grid(x1=xp1, x2=xp2)
x_app.lda <- lda(x_app, classe_app)
grille <- cbind(grille[,1], grille[,2])
Zp <- predict(x_app.lda, grille)
# plot boundaries
zp<-Zp$post[,3]-pmax(Zp$post[,2],Zp$post[,1])
contour(xp1,xp2,matrix(zp,50),levels=0,drawlabels=FALSE)
zp<-Zp$post[,2]-pmax(Zp$post[,3],Zp$post[,1])
contour(xp1,xp2,matrix(zp,50),add=TRUE,levels=0,drawlabels=FALSE)
points(x_app, col=couleur_app, p=19)
\end{lstlisting}

\clearpage

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth - 10pt]{train_boundaries.png}
    \caption{Decision lines on train dataset}
\end{figure}

\paragraph{}
The classification isn't perfect.
For example, the line on the right, separating the green instances from the red ones, could have been higher, in order to include the 3 red instances that are classified as being green by our algorithm.
But that would raise the question of \emph{overfit}.
So, overall, we can say that the algorithm \emph{generalizes} the data well.

\subsection{Testing the linear comination}
\paragraph{}
Now that we have a linear combination between our features, we can see how well it behaves on our test data.

\label{accuracy-calculation}
\begin{lstlisting}[language=R, caption=Predicting with LDA]
assigne_test<-predict(x_app.lda, newdata=x_test)
# Estimation des taux de bonnes classifications
table_classification_test <-table(classe_test, assigne_test$class)
# table of correct class vs. classification
diag(prop.table(table_classification_test, 1))
# total percent correct
taux_bonne_classif_test <-sum(diag(prop.table(table_classification_test)))
\end{lstlisting}

\paragraph{}
The script above tells us that we have an overall accuracy of $88\%$: $85\%$ for the red class, $95\%$ for the green class and $\approx 86\%$ for the blue class.
If we are to plot the test data using the same boundaries as above, it would look like so:

\begin{lstlisting}[language=R, caption=Plotting predictions and true classes]
forms <- 1:n_test
possible_forms <- c(15, 17, 19)
for (i in 1:n_test)
    forms[i] = possible_forms[assigne_test$class[i]]
# replot the grid & decision lines
xp1 <- seq(min(x_test[,1]), max(x_test[,1]), length=50)
xp2 <- seq(min(x_test[,2]), max(x_test[,2]), length=50)
grille <- expand.grid(x1=xp1,x2=xp2)
plot (grille)
zp<-Zp$post[,3]-pmax(Zp$post[,2],Zp$post[,1])
contour(xp1,xp2,matrix(zp,50),levels=0,drawlabels=FALSE)
zp<-Zp$post[,2]-pmax(Zp$post[,3],Zp$post[,1])
contour(xp1,xp2,matrix(zp,50),add=TRUE,levels=0,drawlabels=FALSE)
points(x_test, col=couleur_test, p=forms)
\end{lstlisting}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth - 10pt]{predictions.png}
    \caption{Predictions using LDA}
\end{figure}

\clearpage

\paragraph{}
In the figure above, the colors represent the true class of the instances, and each shape represents one of our predictions.
For example, missclassifications are: blue instances that are not circles, red instances that are not squares and green instances that aren't triangles.
\paragraph{}
Counting the missclassifications mentioned above, we find a total of $9$: $3$ red, $1$ green and $5$ blue.
We have a number of $75$ instances in total: $20$ red, $20$ green and $35$ blue.
That gives as a total error rate of $9/75 = 12\%$.
For each individual class, the errors are: $3/20=15\%$, $1/20=5\%$ and $5/35\approx 14\%$.
That is indeed what our algorithm gave us as well\ref{accuracy-calculation}.


\section{Quadratic discriminant analysis}
\paragraph{}
The \emph{Quadratic Classifier} brings, as an advantage over LDA, more generality.
It still does a linear combination of the features, a feature's value may now be raised to a certain power: $f(x) = x^T*A*x + b^Tx + c$.
This will give us decision lines that will be more complex, since introducing terms like $x^T$ in the equation can help us achieve curves for the boundaries.

\paragraph{}
To use QDA instead of LDA, we simply replace ``lda'' with ``qda'' in the code presented above.
Here's how the results look:

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth - 10pt]{qda_train.png}
    \caption{QDA trained on train dataset}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth - 10pt]{qda_test.png}
    \caption{QDA trained on the test dataset}
\end{figure}

\paragraph{}
This time, we have a total accuracy of $93\%$: $95\%$ for the red instances, $100\%$ for the green ones and $\approx 86\%$ for the blue instances.
Quadratic discriminant analysis is more complex, but at the same time this gives more generalization power to the algorithm.
As we can see, the results using QDA are better than those from LDA.